{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abfa854-41e7-451a-b6e8-649dd085e717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 21 06:53:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:0D:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              20W / 300W |   7622MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094d9c0b-886a-4f84-b181-56f522c650e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from base64 import b64encode\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import subprocess\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "import pytorch_lightning as pl\n",
    "from transformers import logging #CLIPTextModel, CLIPTokenizer,\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34e5c97-b860-4e04-8bd5-11609b97be2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abbd307-2a5a-4d11-aef0-a01ebd6b8aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ControlNet'...\n",
      "remote: Enumerating objects: 1398, done.\u001b[K\n",
      "remote: Counting objects: 100% (477/477), done.\u001b[K\n",
      "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
      "remote: Total 1398 (delta 420), reused 408 (delta 393), pack-reused 921\u001b[K\n",
      "Receiving objects: 100% (1398/1398), 122.40 MiB | 17.04 MiB/s, done.\n",
      "Resolving deltas: 100% (637/637), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/lllyasviel/ControlNet.git\n",
    "!git clone https://github.com/anilbhatt1/ControlNet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abae59b-e1ac-40f2-bf88-08301271023a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af95e08d29641599fd0153a519896dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2-1_512-ema-pruned.ckpt:   0%|          | 0.00/5.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/v2-1_512-ema-pruned.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Do huggingface-cli login and add token via vastai terminal before doing this\n",
    "downloaded_model_path = hf_hub_download(repo_id=\"stabilityai/stable-diffusion-2-1-base\",\n",
    "                                        filename=\"v2-1_512-ema-pruned.ckpt\",\n",
    "                                        use_auth_token=True)\n",
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5e39c2-12f9-4bb6-8b02-812f0210c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da0779-d89f-4122-8e3c-5985d690de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy file using gdown and apt install unzip before running this\n",
    "!unzip -q '/fill50k.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d49ca2f-0fdd-419a-8b4e-f85bd572ca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "!ls '/fill50k/source' | wc -l\n",
    "!ls '/fill50k/target' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c0cce9-4e87-4b2a-926d-9f7813076d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        with open('/prompt_10k.json', 'rt') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        self.width = 512\n",
    "        self.height = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        source_filename = item['source']\n",
    "        target_filename = item['target']\n",
    "        prompt = item['prompt']\n",
    "\n",
    "        source = cv2.imread('/fill50k/' + source_filename)\n",
    "        target = cv2.imread('/fill50k/' + target_filename)\n",
    "\n",
    "        source = cv2.resize(source, (self.width, self.height), interpolation=cv2.INTER_NEAREST)\n",
    "        target = cv2.resize(target, (self.width, self.height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Do not forget that OpenCV read images in BGR order.\n",
    "        source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "        target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize source images to [0, 1].\n",
    "        source = source.astype(np.float32) / 255.0\n",
    "\n",
    "        # Normalize target images to [-1, 1].\n",
    "        target = (target.astype(np.float32) / 127.5) - 1.0\n",
    "\n",
    "        return dict(jpg=target, txt=prompt, hint=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e4f3d8-153b-4b06-aa70-4624d2596cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "fuchsia circle with yellow background\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "print(len(dataset))\n",
    "\n",
    "item = dataset[5234]\n",
    "jpg = item['jpg']\n",
    "txt = item['txt']\n",
    "hint = item['hint']\n",
    "print(txt)\n",
    "print(jpg.shape)\n",
    "print(hint.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beaaa139-852a-4e59-b9bc-832c8584a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f30bef-ae75-42c5-9692-b3c12010861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp '/root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/5ede9e4bf3e3fd1cb0ef2f7a3fff13ee514fdf06/v2-1_512-ema-pruned.ckpt' '/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fae4b1-21fb-49ca-af9f-adf0bf4e9b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ControlNet\n",
      "LICENSE\t\t\t       gradio_pose2image.py\n",
      "README.md\t\t       gradio_scribble2image.py\n",
      "__pycache__\t\t       gradio_scribble2image_interactive.py\n",
      "annotator\t\t       gradio_seg2image.py\n",
      "cldm\t\t\t       image_log\n",
      "config.py\t\t       ldm\n",
      "docs\t\t\t       lightning_logs\n",
      "environment.yaml\t       models\n",
      "font\t\t\t       share.py\n",
      "github_page\t\t       test_imgs\n",
      "gradio_annotator.py\t       tool_add_control.py\n",
      "gradio_canny2image.py\t       tool_add_control_sd21.py\n",
      "gradio_depth2image.py\t       tool_transfer_control.py\n",
      "gradio_fake_scribble2image.py  tutorial_dataset.py\n",
      "gradio_hed2image.py\t       tutorial_dataset_test.py\n",
      "gradio_hough2image.py\t       tutorial_train.py\n",
      "gradio_normal2image.py\t       tutorial_train_sd21.py\n"
     ]
    }
   ],
   "source": [
    "%cd '/ControlNet'\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a5ce99-2b71-41c1-aaa8-7c3b31979d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "create_model-config : {'model': {'target': 'cldm.cldm.ControlLDM', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'jpg', 'cond_stage_key': 'txt', 'control_key': 'hint', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'only_mid_control': False, 'control_stage_config': {'target': 'cldm.cldm.ControlNet', 'params': {'use_checkpoint': True, 'image_size': 32, 'in_channels': 4, 'hint_channels': 3, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'legacy': False}}, 'unet_config': {'target': 'cldm.cldm.ControlledUnetModel', 'params': {'use_checkpoint': True, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder', 'params': {'freeze': True, 'layer': 'penultimate'}}}}}\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded model config from [./models/cldm_v21.yaml]\n",
      "These weights are newly added: logvar\n",
      "These weights are newly added: control_model.zero_convs.0.0.weight\n",
      "These weights are newly added: control_model.zero_convs.0.0.bias\n",
      "These weights are newly added: control_model.zero_convs.1.0.weight\n",
      "These weights are newly added: control_model.zero_convs.1.0.bias\n",
      "These weights are newly added: control_model.zero_convs.2.0.weight\n",
      "These weights are newly added: control_model.zero_convs.2.0.bias\n",
      "These weights are newly added: control_model.zero_convs.3.0.weight\n",
      "These weights are newly added: control_model.zero_convs.3.0.bias\n",
      "These weights are newly added: control_model.zero_convs.4.0.weight\n",
      "These weights are newly added: control_model.zero_convs.4.0.bias\n",
      "These weights are newly added: control_model.zero_convs.5.0.weight\n",
      "These weights are newly added: control_model.zero_convs.5.0.bias\n",
      "These weights are newly added: control_model.zero_convs.6.0.weight\n",
      "These weights are newly added: control_model.zero_convs.6.0.bias\n",
      "These weights are newly added: control_model.zero_convs.7.0.weight\n",
      "These weights are newly added: control_model.zero_convs.7.0.bias\n",
      "These weights are newly added: control_model.zero_convs.8.0.weight\n",
      "These weights are newly added: control_model.zero_convs.8.0.bias\n",
      "These weights are newly added: control_model.zero_convs.9.0.weight\n",
      "These weights are newly added: control_model.zero_convs.9.0.bias\n",
      "These weights are newly added: control_model.zero_convs.10.0.weight\n",
      "These weights are newly added: control_model.zero_convs.10.0.bias\n",
      "These weights are newly added: control_model.zero_convs.11.0.weight\n",
      "These weights are newly added: control_model.zero_convs.11.0.bias\n",
      "These weights are newly added: control_model.input_hint_block.0.weight\n",
      "These weights are newly added: control_model.input_hint_block.0.bias\n",
      "These weights are newly added: control_model.input_hint_block.2.weight\n",
      "These weights are newly added: control_model.input_hint_block.2.bias\n",
      "These weights are newly added: control_model.input_hint_block.4.weight\n",
      "These weights are newly added: control_model.input_hint_block.4.bias\n",
      "These weights are newly added: control_model.input_hint_block.6.weight\n",
      "These weights are newly added: control_model.input_hint_block.6.bias\n",
      "These weights are newly added: control_model.input_hint_block.8.weight\n",
      "These weights are newly added: control_model.input_hint_block.8.bias\n",
      "These weights are newly added: control_model.input_hint_block.10.weight\n",
      "These weights are newly added: control_model.input_hint_block.10.bias\n",
      "These weights are newly added: control_model.input_hint_block.12.weight\n",
      "These weights are newly added: control_model.input_hint_block.12.bias\n",
      "These weights are newly added: control_model.input_hint_block.14.weight\n",
      "These weights are newly added: control_model.input_hint_block.14.bias\n",
      "These weights are newly added: control_model.middle_block_out.0.weight\n",
      "These weights are newly added: control_model.middle_block_out.0.bias\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python3 tool_add_control_sd21.py '/models/v2-1_512-ema-pruned.ckpt' '/models/control_sd21_512_ini.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d99e65-6914-4c7b-ad3d-d8d84f2868bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ControlNet'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/ControlNet')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd331eb-06d7-4085-955c-62247ca48971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from tutorial_dataset import MyDataset\n",
    "from cldm.logger import ImageLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a002d95-b2e8-4565-93e8-f699a9da5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cldm.model import create_model, load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc542265-6566-4fa5-89b2-6ffbf13a0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model-config : {'model': {'target': 'cldm.cldm.ControlLDM', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'jpg', 'cond_stage_key': 'txt', 'control_key': 'hint', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'only_mid_control': False, 'control_stage_config': {'target': 'cldm.cldm.ControlNet', 'params': {'use_checkpoint': True, 'image_size': 32, 'in_channels': 4, 'hint_channels': 3, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'legacy': False}}, 'unet_config': {'target': 'cldm.cldm.ControlledUnetModel', 'params': {'use_checkpoint': True, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder', 'params': {'freeze': True, 'layer': 'penultimate'}}}}}\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded model config from [./models/cldm_v21.yaml]\n"
     ]
    }
   ],
   "source": [
    "resume_path = '/models/control_sd21_512_ini.ckpt'\n",
    "batch_size = 8\n",
    "logger_freq = 300\n",
    "learning_rate = 1e-5\n",
    "sd_locked = True\n",
    "only_mid_control = False\n",
    "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
    "model = create_model('./models/cldm_v21.yaml').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba9e5018-7b0e-4a7d-9d3e-6d5ed366ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state_dict from [/models/control_sd21_512_ini.ckpt]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(load_state_dict(resume_path, location='cpu'))\n",
    "model.learning_rate = learning_rate\n",
    "model.sd_locked = sd_locked\n",
    "model.only_mid_control = only_mid_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bdca607-397e-4b17-88e7-a09ebdbf6cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "logger = ImageLogger(batch_frequency=logger_freq)\n",
    "trainer = pl.Trainer(accelerator=\"auto\", precision=32, callbacks=[logger], max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "533b15d9-a33e-4117-a4ba-89ec157612a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /ControlNet/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                   | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper       | 865 M \n",
      "1 | first_stage_model | AutoencoderKL          | 83.7 M\n",
      "2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M \n",
      "3 | control_model     | ControlNet             | 364 M \n",
      "-------------------------------------------------------------\n",
      "1.2 B     Trainable params\n",
      "437 M     Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,671.302 Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610b034134044cd4b7b4e26a05c78c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "DDIM Sampler:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▏         | 1/50 [00:00<00:31,  1.57it/s]\u001b[A\n",
      "DDIM Sampler:   4%|▍         | 2/50 [00:01<00:29,  1.63it/s]\u001b[A\n",
      "DDIM Sampler:   6%|▌         | 3/50 [00:01<00:26,  1.80it/s]\u001b[A\n",
      "DDIM Sampler:   8%|▊         | 4/50 [00:02<00:27,  1.69it/s]\u001b[A\n",
      "DDIM Sampler:  10%|█         | 5/50 [00:02<00:24,  1.80it/s]\u001b[A\n",
      "DDIM Sampler:  12%|█▏        | 6/50 [00:03<00:23,  1.88it/s]\u001b[A\n",
      "DDIM Sampler:  14%|█▍        | 7/50 [00:03<00:22,  1.93it/s]\u001b[A\n",
      "DDIM Sampler:  16%|█▌        | 8/50 [00:04<00:21,  1.97it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█▊        | 9/50 [00:04<00:20,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  20%|██        | 10/50 [00:05<00:19,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██▏       | 11/50 [00:05<00:19,  2.02it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██▍       | 12/50 [00:06<00:18,  2.03it/s]\u001b[A\n",
      "DDIM Sampler:  26%|██▌       | 13/50 [00:06<00:18,  2.03it/s]\u001b[A\n",
      "DDIM Sampler:  28%|██▊       | 14/50 [00:07<00:17,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  30%|███       | 15/50 [00:07<00:17,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  32%|███▏      | 16/50 [00:08<00:16,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  34%|███▍      | 17/50 [00:08<00:16,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  36%|███▌      | 18/50 [00:09<00:15,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███▊      | 19/50 [00:09<00:15,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  40%|████      | 20/50 [00:10<00:14,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████▏     | 21/50 [00:10<00:14,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████▍     | 22/50 [00:11<00:13,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████▌     | 23/50 [00:11<00:13,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  48%|████▊     | 24/50 [00:12<00:12,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████     | 25/50 [00:12<00:12,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  52%|█████▏    | 26/50 [00:13<00:11,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  54%|█████▍    | 27/50 [00:13<00:11,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  56%|█████▌    | 28/50 [00:14<00:10,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  58%|█████▊    | 29/50 [00:14<00:10,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  60%|██████    | 30/50 [00:15<00:09,  2.04it/s]\u001b[A\n",
      "DDIM Sampler:  62%|██████▏   | 31/50 [00:15<00:09,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████▍   | 32/50 [00:16<00:08,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  66%|██████▌   | 33/50 [00:16<00:08,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████▊   | 34/50 [00:17<00:07,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  70%|███████   | 35/50 [00:17<00:07,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  72%|███████▏  | 36/50 [00:18<00:06,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  74%|███████▍  | 37/50 [00:18<00:06,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  76%|███████▌  | 38/50 [00:18<00:05,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  78%|███████▊  | 39/50 [00:19<00:05,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  80%|████████  | 40/50 [00:19<00:04,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  82%|████████▏ | 41/50 [00:20<00:04,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████▍ | 42/50 [00:20<00:03,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████▌ | 43/50 [00:21<00:03,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  88%|████████▊ | 44/50 [00:21<00:02,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  90%|█████████ | 45/50 [00:22<00:02,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  92%|█████████▏| 46/50 [00:22<00:01,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  94%|█████████▍| 47/50 [00:23<00:01,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  96%|█████████▌| 48/50 [00:23<00:00,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  98%|█████████▊| 49/50 [00:24<00:00,  2.05it/s]\u001b[A\n",
      "DDIM Sampler: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "DDIM Sampler:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▏         | 1/50 [00:00<00:24,  2.03it/s]\u001b[A\n",
      "DDIM Sampler:   4%|▍         | 2/50 [00:00<00:23,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:   6%|▌         | 3/50 [00:01<00:23,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:   8%|▊         | 4/50 [00:01<00:22,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  10%|█         | 5/50 [00:02<00:22,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  12%|█▏        | 6/50 [00:02<00:21,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  14%|█▍        | 7/50 [00:03<00:21,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  16%|█▌        | 8/50 [00:03<00:20,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█▊        | 9/50 [00:04<00:20,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  20%|██        | 10/50 [00:04<00:19,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██▏       | 11/50 [00:05<00:19,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██▍       | 12/50 [00:05<00:18,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  26%|██▌       | 13/50 [00:06<00:18,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  28%|██▊       | 14/50 [00:06<00:17,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  30%|███       | 15/50 [00:07<00:17,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  32%|███▏      | 16/50 [00:07<00:16,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  34%|███▍      | 17/50 [00:08<00:16,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  36%|███▌      | 18/50 [00:08<00:15,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███▊      | 19/50 [00:09<00:15,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  40%|████      | 20/50 [00:09<00:14,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████▏     | 21/50 [00:10<00:14,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████▍     | 22/50 [00:10<00:13,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████▌     | 23/50 [00:11<00:13,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  48%|████▊     | 24/50 [00:11<00:12,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████     | 25/50 [00:12<00:12,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  52%|█████▏    | 26/50 [00:12<00:11,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  54%|█████▍    | 27/50 [00:13<00:11,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  56%|█████▌    | 28/50 [00:13<00:10,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  58%|█████▊    | 29/50 [00:14<00:10,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  60%|██████    | 30/50 [00:14<00:09,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  62%|██████▏   | 31/50 [00:15<00:09,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████▍   | 32/50 [00:15<00:08,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  66%|██████▌   | 33/50 [00:16<00:08,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████▊   | 34/50 [00:16<00:07,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  70%|███████   | 35/50 [00:17<00:07,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  72%|███████▏  | 36/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  74%|███████▍  | 37/50 [00:18<00:06,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  76%|███████▌  | 38/50 [00:18<00:05,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  78%|███████▊  | 39/50 [00:19<00:05,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  80%|████████  | 40/50 [00:19<00:04,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  82%|████████▏ | 41/50 [00:20<00:04,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████▍ | 42/50 [00:20<00:03,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████▌ | 43/50 [00:21<00:03,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  88%|████████▊ | 44/50 [00:21<00:02,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  90%|█████████ | 45/50 [00:22<00:02,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  92%|█████████▏| 46/50 [00:22<00:01,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  94%|█████████▍| 47/50 [00:23<00:01,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  96%|█████████▌| 48/50 [00:23<00:00,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  98%|█████████▊| 49/50 [00:24<00:00,  2.01it/s]\u001b[A\n",
      "DDIM Sampler: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "DDIM Sampler:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▏         | 1/50 [00:00<00:24,  2.03it/s]\u001b[A\n",
      "DDIM Sampler:   4%|▍         | 2/50 [00:00<00:23,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:   6%|▌         | 3/50 [00:01<00:23,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:   8%|▊         | 4/50 [00:01<00:22,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  10%|█         | 5/50 [00:02<00:22,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  12%|█▏        | 6/50 [00:02<00:21,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  14%|█▍        | 7/50 [00:03<00:21,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  16%|█▌        | 8/50 [00:03<00:20,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█▊        | 9/50 [00:04<00:20,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  20%|██        | 10/50 [00:04<00:19,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██▏       | 11/50 [00:05<00:19,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██▍       | 12/50 [00:05<00:18,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  26%|██▌       | 13/50 [00:06<00:18,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  28%|██▊       | 14/50 [00:06<00:17,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  30%|███       | 15/50 [00:07<00:17,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  32%|███▏      | 16/50 [00:07<00:16,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  34%|███▍      | 17/50 [00:08<00:16,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  36%|███▌      | 18/50 [00:08<00:15,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  38%|███▊      | 19/50 [00:09<00:15,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  40%|████      | 20/50 [00:09<00:14,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  42%|████▏     | 21/50 [00:10<00:14,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████▍     | 22/50 [00:10<00:13,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████▌     | 23/50 [00:11<00:13,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  48%|████▊     | 24/50 [00:11<00:12,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████     | 25/50 [00:12<00:12,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  52%|█████▏    | 26/50 [00:12<00:11,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  54%|█████▍    | 27/50 [00:13<00:11,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  56%|█████▌    | 28/50 [00:13<00:10,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  58%|█████▊    | 29/50 [00:14<00:10,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  60%|██████    | 30/50 [00:14<00:09,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  62%|██████▏   | 31/50 [00:15<00:09,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████▍   | 32/50 [00:15<00:08,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  66%|██████▌   | 33/50 [00:16<00:08,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████▊   | 34/50 [00:16<00:07,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  70%|███████   | 35/50 [00:17<00:07,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  72%|███████▏  | 36/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  74%|███████▍  | 37/50 [00:18<00:06,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  76%|███████▌  | 38/50 [00:18<00:05,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  78%|███████▊  | 39/50 [00:19<00:05,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  80%|████████  | 40/50 [00:19<00:04,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  82%|████████▏ | 41/50 [00:20<00:04,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████▍ | 42/50 [00:20<00:03,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  86%|████████▌ | 43/50 [00:21<00:03,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  88%|████████▊ | 44/50 [00:21<00:02,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  90%|█████████ | 45/50 [00:22<00:02,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  92%|█████████▏| 46/50 [00:22<00:01,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  94%|█████████▍| 47/50 [00:23<00:01,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  96%|█████████▌| 48/50 [00:23<00:00,  2.01it/s]\u001b[A\n",
      "DDIM Sampler:  98%|█████████▊| 49/50 [00:24<00:00,  2.01it/s]\u001b[A\n",
      "DDIM Sampler: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1032\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1032\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:206\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:378\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    377\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:323\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    321\u001b[0m monitor_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_monitor_candidates(trainer)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_topk_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_last_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:385\u001b[0m, in \u001b[0;36mModelCheckpoint._save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_monitor_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_none_monitor_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_candidates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:713\u001b[0m, in \u001b[0;36mModelCheckpoint._save_none_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# set the best model path before saving because it will be part of the state.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m previous, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model_path, filepath\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_top_k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m previous \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_remove_checkpoint(trainer, previous, filepath):\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_checkpoint(trainer, previous)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:388\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, filepath: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_global_step_saved \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_checkpoint_saved \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1380\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving a checkpoint is only possible if a model is attached to the Trainer. Did you call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer.save_checkpoint()` before calling `Trainer.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mfit,validate,test,predict}`?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m     )\n\u001b[1;32m   1379\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m-> 1380\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.save_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:491\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning_fabric/plugins/io/torch_io.py:58\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path)\n\u001b[1;32m     57\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_atomic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:79\u001b[0m, in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     77\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(checkpoint, bytesbuffer)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fsspec\u001b[38;5;241m.\u001b[39mopen(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytesbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:369\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19a764-d626-4424-b5d3-b67f40960bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
