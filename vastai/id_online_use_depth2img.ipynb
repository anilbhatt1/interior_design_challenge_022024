{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710503f2-ad3e-4fe4-9e92-9c671c5c2532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Union, List\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import ControlNetModel\n",
    "from diffusers.pipelines.controlnet import StableDiffusionControlNetInpaintPipeline, StableDiffusionControlNetPipeline\n",
    "from diffusers import ControlNetModel, UniPCMultistepScheduler\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation, CLIPTextModel, CLIPTokenizer, DataCollatorWithPadding, pipeline\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionInpaintPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "from models.colors import ade_palette\n",
    "from models.utils import map_colors_rgb\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import StableDiffusionDepth2ImgPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31637de5-15a9-4a4c-b9a7-4fed460a8868",
   "metadata": {},
   "source": [
    "CLIP MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d782302-2222-4c37-a07e-d1e40a0cc3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    subfolder=\"text_encoder\") \n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    subfolder=\"tokenizer\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814971c8-387a-49f5-984f-af47b6cab6c5",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8679333d-72c0-4c42-b97d-2e4001f28007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_items(\n",
    "    colors_list: Union[List, np.ndarray],\n",
    "    items_list: Union[List, np.ndarray],\n",
    "    items_to_retain: Union[List, np.ndarray]\n",
    ") -> Tuple[Union[List, np.ndarray], Union[List, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Filters items and their corresponding colors from given lists, excluding\n",
    "    specified items.\n",
    "\n",
    "    Args:\n",
    "        colors_list: A list or numpy array of colors corresponding to items.\n",
    "        items_list: A list or numpy array of items.\n",
    "        items_to_remove: A list or numpy array of items to be removed.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two lists or numpy arrays: filtered colors and filtered\n",
    "        items.\n",
    "    \"\"\"\n",
    "    filtered_colors = []\n",
    "    filtered_items = []\n",
    "    for color, item in zip(colors_list, items_list):\n",
    "        if item in items_to_retain:\n",
    "            filtered_colors.append(color)\n",
    "            filtered_items.append(item)\n",
    "    return filtered_colors, filtered_items\n",
    "\n",
    "def filter_items_mask(colors_list,items_list,items_to_mask):\n",
    "    \"\"\"\n",
    "    Filters items and their corresponding colors from given lists, excluding\n",
    "    specified items.\n",
    "\n",
    "    Args:\n",
    "        colors_list: A list or numpy array of colors corresponding to items.\n",
    "        items_list: A list or numpy array of items.\n",
    "        items_to_remove: A list or numpy array of items to be removed.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two lists or numpy arrays: filtered colors and filtered\n",
    "        items.\n",
    "    \"\"\"\n",
    "    filtered_colors = []\n",
    "    filtered_items = []\n",
    "    for color, item in zip(colors_list, items_list):\n",
    "        if item not in items_to_mask:\n",
    "            filtered_colors.append(color)\n",
    "            filtered_items.append(item)\n",
    "    return filtered_colors, filtered_items\n",
    "\n",
    "def filter_items_retain(colors_list,items_list,items_to_retain):\n",
    "    \"\"\"\n",
    "    Filters items and their corresponding colors from given lists, excluding\n",
    "    specified items.\n",
    "\n",
    "    Args:\n",
    "        colors_list: A list or numpy array of colors corresponding to items.\n",
    "        items_list: A list or numpy array of items.\n",
    "        items_to_remove: A list or numpy array of items to be removed.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two lists or numpy arrays: filtered colors and filtered\n",
    "        items.\n",
    "    \"\"\"\n",
    "    filtered_colors = []\n",
    "    filtered_items = []\n",
    "    for color, item in zip(colors_list, items_list):\n",
    "        if item in items_to_retain:\n",
    "            filtered_colors.append(color)\n",
    "            filtered_items.append(item)\n",
    "    return filtered_colors, filtered_items\n",
    "\n",
    "def get_segmentation_pipeline(\n",
    ") -> Tuple[AutoImageProcessor, UperNetForSemanticSegmentation]:\n",
    "    \"\"\"Method to load the segmentation pipeline\n",
    "    Returns:\n",
    "        Tuple[AutoImageProcessor, UperNetForSemanticSegmentation]: segmentation pipeline\n",
    "    \"\"\"\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "         \"openmmlab/upernet-convnext-small\"\n",
    "    )\n",
    "    image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\n",
    "        \"openmmlab/upernet-convnext-small\"\n",
    "    )\n",
    "    return image_processor, image_segmentor\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "@torch.autocast('cuda')\n",
    "def segment_image(\n",
    "        image: Image,\n",
    "        image_processor: AutoImageProcessor,\n",
    "        image_segmentor: UperNetForSemanticSegmentation\n",
    ") -> Image:\n",
    "    \"\"\"\n",
    "    Segments an image using a semantic segmentation model.\n",
    "\n",
    "    Args:\n",
    "        image (Image): The input image to be segmented.\n",
    "        image_processor (AutoImageProcessor): The processor to prepare the\n",
    "            image for segmentation.\n",
    "        image_segmentor (UperNetForSemanticSegmentation): The semantic\n",
    "            segmentation model used to identify different segments in the image.\n",
    "\n",
    "    Returns:\n",
    "        Image: The segmented image with each segment colored differently based\n",
    "            on its identified class.\n",
    "    \"\"\"\n",
    "    # image_processor, image_segmentor = get_segmentation_pipeline()\n",
    "    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "    with torch.no_grad():\n",
    "        outputs = image_segmentor(pixel_values)\n",
    "\n",
    "    seg = image_processor.post_process_semantic_segmentation(\n",
    "        outputs, target_sizes=[image.size[::-1]])[0]\n",
    "    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
    "    palette = np.array(ade_palette())\n",
    "    for label, color in enumerate(palette):\n",
    "        color_seg[seg == label, :] = color\n",
    "    color_seg = color_seg.astype(np.uint8)\n",
    "    seg_image = Image.fromarray(color_seg).convert('RGB')\n",
    "    return seg_image\n",
    "\n",
    "def resize_dimensions(dimensions, target_size):\n",
    "    \"\"\" \n",
    "    Resize PIL to target size while maintaining aspect ratio \n",
    "    If smaller than target size leave it as is\n",
    "    \"\"\"\n",
    "    width, height = dimensions\n",
    "\n",
    "    # Check if both dimensions are smaller than the target size\n",
    "    if width < target_size and height < target_size:\n",
    "        return dimensions\n",
    "\n",
    "    # Determine the larger side\n",
    "    if width > height:\n",
    "        # Calculate the aspect ratio\n",
    "        aspect_ratio = height / width\n",
    "        # Resize dimensions\n",
    "        return (target_size, int(target_size * aspect_ratio))\n",
    "    else:\n",
    "        # Calculate the aspect ratio\n",
    "        aspect_ratio = width / height\n",
    "        # Resize dimensions\n",
    "        return (int(target_size * aspect_ratio), target_size)\n",
    "\n",
    "def tokenize_function(caption):\n",
    "    return tokenizer(caption, truncation=False)\n",
    "\n",
    "def do_encode(inputs, text_encoder, device, max_seq_len=75):\n",
    "    embeddings = []\n",
    "    tokens = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    num_chunks = (tokens.size(1) + max_seq_len - 1) // max_seq_len\n",
    "\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    tokens = tokens.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * max_seq_len\n",
    "        end_idx = start_idx + max_seq_len\n",
    "        chunk_tokens = tokens[:, start_idx:end_idx]\n",
    "        # chunk_attention_mask = attention_mask[:, start_idx:end_idx]\n",
    "\n",
    "        chunk_embeddings = text_encoder.text_model.embeddings.token_embedding(chunk_tokens)\n",
    "\n",
    "        chunk_size = chunk_tokens.size(1)\n",
    "        position_ids = torch.arange(start_idx, start_idx + chunk_size, dtype=torch.long)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(chunk_tokens.size(0), chunk_size)\n",
    "\n",
    "        position_ids = torch.clamp(position_ids.to(device), max=text_encoder.text_model.embeddings.position_embedding.num_embeddings - 1)\n",
    "        position_embeddings = text_encoder.text_model.embeddings.position_embedding(position_ids)\n",
    "        chunk_embeddings += position_embeddings\n",
    "\n",
    "        embeddings.append(chunk_embeddings)\n",
    "\n",
    "    concatenated_embeddings = torch.cat(embeddings, dim=1)\n",
    "    attention_mask_expanded = attention_mask.unsqueeze(1).unsqueeze(2).repeat(1, 1, attention_mask.shape[1], 1)\n",
    "    encoder_outputs = text_encoder.text_model.encoder(concatenated_embeddings, attention_mask=attention_mask_expanded)\n",
    "    return (encoder_outputs.last_hidden_state)\n",
    "    # return encoder_outputs[0]\n",
    "\n",
    "def get_pipeline_embeds_mod(input_ids, negative_ids):\n",
    "    max_length = tokenizer.model_max_length\n",
    "    shape_max_length = max(input_ids.shape[-1], negative_ids.shape[-1])                                 \n",
    "    concat_embeds = []\n",
    "    neg_embeds = []\n",
    "    for i in range(0, shape_max_length, max_length):\n",
    "        concat_embeds.append(text_encoder(input_ids[:, i: i + max_length])[0])\n",
    "        neg_embeds.append(text_encoder(negative_ids[:, i: i + max_length])[0])\n",
    "    return torch.cat(concat_embeds, dim=1), torch.cat(neg_embeds, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3483e-511e-4728-b981-8059ebf678fb",
   "metadata": {},
   "source": [
    "MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25bd164-3ca6-42f0-8778-38abab77e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlNetDesignModel_wall_mask:\n",
    "    \"\"\" Produces random noise images \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize your model(s) here \"\"\"\n",
    "\n",
    "        os.environ['HF_HUB_OFFLINE'] = \"False\"\n",
    "\n",
    "        unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"unet\")\n",
    "                    \n",
    "        unet.requires_grad_(False)\n",
    "        weight_dtype = torch.float32\n",
    "        unet.to('cuda', dtype=weight_dtype)\n",
    "\n",
    "        lora_attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            # print(f'name in unet : {name}')\n",
    "            cross_attention_dim = (\n",
    "                None\n",
    "                if name.endswith(\"attn1.processor\")\n",
    "                else unet.config.cross_attention_dim\n",
    "            )\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "            lora_attn_procs[name] = LoRAAttnProcessor(\n",
    "                hidden_size=hidden_size,\n",
    "                cross_attention_dim=cross_attention_dim,\n",
    "                rank=64,\n",
    "            )\n",
    "            \n",
    "        unet.set_attn_processor(lora_attn_procs)\n",
    "        lora_layers = AttnProcsLayers(unet.attn_processors)    \n",
    "\n",
    "        ### NEW CODE - DEPTH2IMG ###\n",
    "        self.depth2img_pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-2-depth\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.depth2img_pipe.to(\"cuda\")\n",
    "        #######\n",
    "\n",
    "        controlnet_seg = ControlNetModel.from_pretrained(\n",
    "            \"BertChristiaens/controlnet-seg-room\", torch_dtype=torch.float32)\n",
    "\n",
    "        self.pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-inpainting\",\n",
    "            controlnet=controlnet_seg,\n",
    "            safety_checker=None,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()\n",
    "        self.pipe = self.pipe.to(\"cuda\")\n",
    "\n",
    "        # customised unet\n",
    "        # /models/unet_fine_tuned_weights/root/stable/output/pytorch_lora_weights.safetensors\n",
    "        unet_weight_path = \"/models/unet_fine_tuned_weights/root/stable/output/pytorch_lora_weights.safetensors\"\n",
    "        #unet_weight_path = \"models/unet_fine_tuned_weights/pytorch_lora_weights_run1403.safetensors\"\n",
    "        self.pipe.unet.load_attn_procs(unet_weight_path, use_safetensors=True)\n",
    "\n",
    "        self.seg_image_processor, self.image_segmentor = get_segmentation_pipeline()\n",
    "\n",
    "        self.seed = 2\n",
    "        self.neg_prompt = \"lowres, watermark, banner, logo, contactinfo, text, deformed, blurry, blur, \\\n",
    "        out of focus, out of frame, surreal, ugly, distortion, low-res, poor quality, \"\n",
    "        self.additional_quality_suffix = \"interior design, 4K, high resolution\"        \n",
    "        self.control_items = [\"floor;flooring\", \"rug;carpet;carpeting\", \"wall\", \"ceiling\"]\n",
    "        self.control_items_mask = [\"stairs;steps\", \"step;stair\", \"stairway;staircase\", \"radiator\", \"screen;door;screen\", \"windowpane;window\", \"door;double;door\", \"countertop\", \"fireplace;hearth;open;fireplace\",\"column;pillar\"]\n",
    "        self.control_items_retain = [\"floor;flooring\", \"rug;carpet;carpeting\", \"wall\", \"ceiling\"]\n",
    "\n",
    "    def generate_design(self, empty_room_image: Image, prompt: str) -> Image:\n",
    "        \"\"\"\n",
    "        Given an image of an empty room and a prompt\n",
    "        generate the designed room according to the prompt\n",
    "        Inputs - \n",
    "            empty_room_image - An RGB PIL Image of the empty room\n",
    "            prompt - Text describing the target design elements of the room\n",
    "        Returns - \n",
    "            design_image - PIL Image of the same size as the empty room image\n",
    "                           If the size is not the same the submission will fail.\n",
    "        \"\"\"            \n",
    "\n",
    "\n",
    "        ## prompt - embeddings\n",
    "        pos_prompt = prompt + f', {self.additional_quality_suffix},'\n",
    "        prompt_lst = [pos_prompt, self.neg_prompt]\n",
    "        prompt_token_lst = []\n",
    "        for prompt in prompt_lst:\n",
    "            prompt_dict = tokenize_function(prompt)\n",
    "            prompt_token_lst.append(prompt_dict)\n",
    "        prompt_tensors = data_collator(prompt_token_lst)\n",
    "        prompt_ids = prompt_tensors['input_ids']\n",
    "        pos_prompt_ids = prompt_ids[0, :].unsqueeze(0)\n",
    "        neg_prompt_ids = prompt_ids[1, :].unsqueeze(0)\n",
    "        pos_prompt_embed, neg_prompt_embed = get_pipeline_embeds_mod(pos_prompt_ids, neg_prompt_ids) \n",
    "\n",
    "        \n",
    "        ### NEW CODE - depth to image ####\n",
    "        depth2img = self.depth2img_pipe(prompt=pos_prompt,\n",
    "            negative_prompt=self.neg_prompt, image=empty_room_image, strength=0.75).images[0]\n",
    "        \n",
    "        depth2img_np = np.array(depth2img)\n",
    "        input_image = Image.fromarray(depth2img_np).convert(\"RGB\")\n",
    "\n",
    "        # image resizing ( after depth)\n",
    "        orig_w, orig_h = input_image.size\n",
    "        new_width, new_height = resize_dimensions(input_image.size, 768)\n",
    "        input_image = input_image.resize((new_width, new_height))\n",
    "        image_np = np.array(input_image)\n",
    "        image = Image.fromarray(image_np).convert(\"RGB\")\n",
    "        print(f\"Image after depth2img\")\n",
    "        image.show()\n",
    "        #################\n",
    "\n",
    "        # segment image - first segmentation model\n",
    "        real_seg = np.array(segment_image(input_image,\n",
    "                                          self.seg_image_processor,\n",
    "                                          self.image_segmentor))\n",
    "        unique_colors = np.unique(real_seg.reshape(-1, real_seg.shape[2]), axis=0)\n",
    "        unique_colors = [tuple(color) for color in unique_colors]\n",
    "        segment_items = [map_colors_rgb(i) for i in unique_colors]\n",
    "        chosen_colors, segment_items_1 = filter_items_mask(\n",
    "            colors_list=unique_colors,\n",
    "            items_list=segment_items,\n",
    "            items_to_mask=self.control_items_mask\n",
    "        )\n",
    "        mask = np.zeros_like(real_seg)\n",
    "        for color in chosen_colors:\n",
    "            color_matches = (real_seg == color).all(axis=2)\n",
    "            mask[color_matches] = 1\n",
    "\n",
    "        # segmented image\n",
    "        segmentation_cond_image = Image.fromarray(real_seg).convert(\"RGB\")\n",
    "        mask_image = Image.fromarray((mask * 255).astype(np.uint8)).convert(\"RGB\")\n",
    "\n",
    "        mask_0_array = (mask * 255).astype(np.uint8)\n",
    "        mask_1_image = Image.fromarray(mask_0_array).convert(\"L\")\n",
    "        mask_1_array = np.array(mask_1_image)\n",
    "\n",
    "        object_items_2 = [\"wall\"]\n",
    "        chosen_colors_2, segment_items_2 = filter_items_mask(\n",
    "            colors_list=unique_colors,\n",
    "            items_list=segment_items,\n",
    "            items_to_mask=object_items_2,\n",
    "        )                \n",
    "        mask_2 = np.zeros_like(real_seg)\n",
    "        for color in chosen_colors_2:\n",
    "            color_matches = (real_seg == color).all(axis=2)\n",
    "            mask_2[color_matches] = 1   \n",
    "            \n",
    "        mask_2_array = (mask_2 * 255).astype(np.uint8)\n",
    "        mask_2_image = Image.fromarray(mask_2_array).convert(\"L\")\n",
    "\n",
    "        # Find the wall height for each column of the image\n",
    "        mask_3_array = np.array(mask_2_image)\n",
    "        wall_heights = []\n",
    "        for col in range(mask_3_array.shape[1]):\n",
    "            # Find the black pixelsfrom the top of the column\n",
    "            black_indices = np.nonzero(mask_3_array[:, col] == 0)[0]\n",
    "            if black_indices.size == 0:\n",
    "                min_ = 0\n",
    "                max_ = 6\n",
    "            else:\n",
    "                max_ = max(black_indices)\n",
    "                min_ = min(black_indices)            \n",
    "            tup = (min_, max_)\n",
    "            wall_heights.append(tup)\n",
    "    \n",
    "        height, width = mask_3_array.shape\n",
    "        white_image_array = np.full((height, width), 255, dtype=np.uint8)\n",
    "    \n",
    "        for col_idx, coords in enumerate(wall_heights):\n",
    "            min_, max_ = coords\n",
    "            wall_ht = max_ - min_\n",
    "            mask_wall_ht = int(0.15 * (wall_ht)) \n",
    "            new_max_ = min_ + mask_wall_ht\n",
    "            for col in range(white_image_array.shape[1]):\n",
    "                white_image_array[min_: new_max_, col_idx] = 0    \n",
    "        \n",
    "        combined_mask_array = cv2.bitwise_and(mask_1_array, white_image_array)  \n",
    "        final_mask_image = Image.fromarray((combined_mask_array).astype(np.uint8)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        # pipeline\n",
    "        generated_image = self.pipe(\n",
    "            prompt_embeds=pos_prompt_embed,\n",
    "            negative_prompt_embeds=neg_prompt_embed,\n",
    "            num_inference_steps=50,\n",
    "            strength=1.0,\n",
    "            guidance_scale=7.0,\n",
    "            generator=[torch.Generator(device=\"cuda\").manual_seed(self.seed)],\n",
    "            image=image,\n",
    "            mask_image=final_mask_image,\n",
    "            control_image=segmentation_cond_image,\n",
    "        ).images[0]\n",
    "\n",
    "        design_image = generated_image.resize(\n",
    "            (orig_w, orig_h), Image.Resampling.LANCZOS\n",
    "        )\n",
    "        \n",
    "        return design_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b0ea3-c738-4212-a8a1-5c7263d1c6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2702eb324a674a6f8bc0ca5ab189c1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#inistalise\n",
    "ctl = ControlNetDesignModel_wall_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5fb2a-24eb-4d5d-8885-f38ee99626cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1 image\n",
    "img = load_image('image_0.jpg')\n",
    "img.show()\n",
    "prompt = \"A Bauhaus-inspired living room with a sleek black leather sofa, a tubular steel coffee table exemplifying modernist design, and a geometric patterned rug adding a touch of artistic flair.\"\n",
    "ctl.generate_design(img,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef86380-3687-47bd-995d-1a1aa87f0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of images\n",
    "\n",
    "captions_csv = '/development_data-v0.2/all_input_list.csv'\n",
    "file1 = open(captions_csv, 'r')\n",
    "Lines = file1.readlines()\n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        continue\n",
    "    sep_line = line.strip().split('.jpg')\n",
    "    img_name = '/development_data-v0.2/' + sep_line[0] + '.jpg'\n",
    "    prompt = sep_line[1].strip()\n",
    "    print(f\"Processing Image:: {img_name}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    img = load_image(img_name)\n",
    "    img.show()\n",
    "    designed_img = ctl.generate_design(img,prompt)\n",
    "    designed_img.show()\n",
    "    print(f'Processed ALL images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28127858-19f9-49e6-b15d-aa4bf3ec4164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
